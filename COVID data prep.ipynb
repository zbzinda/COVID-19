{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes you through the necessary data transformations to produce a basic COVID-19 dashboard using data available on the Johns Hopkins Github page and through the Twitter API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import numpy as np\n",
    "from twython import Twython\n",
    "import json\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time running this notebook you can skip the next block of code. This block simply deletes old data from the last time I updated the dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"C:\\\\Users\\\\zacha\\\\Downloads\\\\COVID-19-master.zip\") # replace with your file path\n",
    "shutil.rmtree(\"C:\\\\Users\\\\zacha\\\\covid_dashboard\") # replace with your file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available for download at this link https://github.com/CSSEGISandData/COVID-19 The block of code below just creates the directory where you want to store your data. Update the file paths as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\zacha\")\n",
    "!mkdir covid_dashboard\n",
    "\n",
    "\n",
    "zippedfile = \"C:\\\\Users\\\\zacha\\\\Downloads\\\\COVID-19-master.zip\"\n",
    "destination_directory = \"C:\\\\Users\\\\zacha\\\\covid_dashboard\"\n",
    "\n",
    "with ZipFile(zippedfile, 'r') as zipObj:\n",
    "   zipObj.extractall(destination_directory)\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\zacha\\\\covid_dashboard\\\\COVID-19-master\\\\csse_covid_19_data\\\\csse_covid_19_time_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block will make a table that will allow you to create a visual where it is possible to compare countries on different metrics. For example, we could look at confirmed cases, deaths, and recoveries for just the U.S. or put the U.S., Italy, and Spain all on the same graph at the same time. Since the standard Power BI line chart does not support multiple legends (i.e., different linetypes for deaths, cases, and recoveries and different colors for each country) we'll create a column called \"measure\" which combines both the country/region and the virus metric. Johns Hopkins has a made a few changes with file names and how data is reported as the situation has evolved, but this is current as of mid-April. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the 3 different files we are pulling data from\n",
    "files = ['confirmed_global', 'deaths_global', 'recovered_global']\n",
    "\n",
    "dataframes = []\n",
    "for a_file in files:\n",
    "    \n",
    "    df = pd.read_csv(\"time_series_covid19_\" + a_file + \".csv\")\n",
    "    \n",
    "    # Australia, China, and Canada have province level data but do not have a row for whole country totals\n",
    "    df = df[df['Province/State'].isna() | df['Country/Region'].isin(['Australia', 'China', 'Canada'])]\n",
    "\n",
    "    df = pd.melt(df, id_vars =['Province/State', 'Country/Region', 'Lat', 'Long'],\n",
    "             var_name ='Date', value_name = a_file[:-7])\n",
    "    \n",
    "    dataframes.append(df)\n",
    "\n",
    "df = dataframes[0].merge(dataframes[1], how = 'left', on = ['Province/State', 'Country/Region', 'Lat', 'Long', 'Date'])\\\n",
    "                  .merge(dataframes[2], how = 'left', on = ['Province/State', 'Country/Region', 'Lat', 'Long', 'Date'])\n",
    "\n",
    "# sum Australian, Chinese, and Canadian data to get country totals\n",
    "df2 = df[df['Country/Region'].isin(['Australia', 'China', 'Canada'])]\\\n",
    "        .groupby(['Country/Region', 'Date'])\\\n",
    "        .sum()\\\n",
    "        .reset_index()\\\n",
    "        .drop(columns = ['Lat', 'Long'])\n",
    "\n",
    "df = df[df['Province/State'].isna()]\n",
    "\n",
    "df.drop(columns = ['Province/State', 'Lat', 'Long'], inplace = True)\n",
    "\n",
    "df = df.append(df2)\n",
    "\n",
    "df = pd.melt(df, id_vars = ['Country/Region', 'Date'], var_name = 'Measure', value_name = 'Total')\n",
    "\n",
    "df.loc[df.Measure == 'confirmed', 'Measure'] = \"Confirmed Cases\"\n",
    "df.loc[df.Measure == 'deaths', 'Measure'] = \"Deaths\"\n",
    "df.loc[df.Measure == 'recovered', 'Measure'] = \"Recoveries\"\n",
    "\n",
    "df['Country/Region and Measure'] = df['Country/Region'] + \" - \" + df['Measure']\n",
    "\n",
    "# we'll calculate a \"daily increase\" in deaths/cases/recoveries for each day and country for an additional visual\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df['Previous_Day_Total'] = df.sort_values(by=['Country/Region', 'Measure', 'Date'], ascending=True)\\\n",
    "                             .groupby(['Country/Region', 'Measure', 'Country/Region and Measure'])['Total']\\\n",
    "                             .shift(1)\n",
    "\n",
    "df['Previous_Day_Total'] = df['Previous_Day_Total'].fillna(0)\n",
    "\n",
    "df['Daily Increase'] = df['Total'] - df['Previous_Day_Total']\n",
    "\n",
    "df.drop(columns = ['Previous_Day_Total'], inplace = True)\n",
    "\n",
    "\n",
    "df.to_csv('joined_global_time_series.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block gets current country totals which allows you to easily make a world map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = df[df.Date == max(df.Date)]\\\n",
    "    .drop(columns = ['Country/Region and Measure', 'Date', 'Daily Increase'])\\\n",
    "    .pivot_table(values='Total', index='Country/Region', columns='Measure')\\\n",
    "    .reset_index()\n",
    "\n",
    "df_current['Natural Log of Current Cases'] = np.log(df_current['Confirmed Cases'] + 1)\n",
    "\n",
    "\n",
    "df_current.to_csv('current_global_cases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will produce totals at a state level and the one day change. This will allow you to make a table similar to what is on the \"worldometer\" COVID website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USc = pd.read_csv(\"time_series_covid19_confirmed_US.csv\")\n",
    "df_USc.drop(columns = ['UID', 'code3', 'FIPS', 'Lat', 'Long_'], inplace = True)\n",
    "df_USc = df_USc.groupby(['Province_State', 'Country_Region']).sum().reset_index()\n",
    "df_USc.rename(columns = {df_USc.columns[-1]: \"Confirmed Cases\",\n",
    "                         df_USc.columns[-2]: \"Yesterday Confirmed Cases\"}, inplace = True)\n",
    "df_USc = df_USc[['Province_State', 'Confirmed Cases', 'Yesterday Confirmed Cases']]\n",
    "df_USc['New Cases Yesterday'] = df_USc['Confirmed Cases'] - df_USc['Yesterday Confirmed Cases']\n",
    "\n",
    "df_USd = pd.read_csv(\"time_series_covid19_deaths_US.csv\")\n",
    "df_USd.drop(columns = ['UID', 'code3', 'FIPS', 'Lat', 'Long_'], inplace = True)\n",
    "df_USd = df_USd.groupby(['Province_State', 'Country_Region']).sum().reset_index()\n",
    "df_USd.rename(columns = {df_USd.columns[-1]: \"Deaths\",\n",
    "                         df_USd.columns[-2]: \"Yesterday Deaths\"}, inplace = True)\n",
    "df_USd = df_USd[['Province_State', 'Deaths', 'Yesterday Deaths']]\n",
    "df_USd['New Deaths Yesterday'] = df_USd['Deaths'] - df_USd['Yesterday Deaths']\n",
    "\n",
    "\n",
    "df_US = pd.merge(df_USc, df_USd, how = 'left')\n",
    "df_US = df_US[['Province_State', 'Confirmed Cases', 'New Cases Yesterday', 'Deaths', 'New Deaths Yesterday']]\n",
    "df_US.to_csv('current_US_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will allow you to make a filled map with county level data. I've filtered by FIPS to remove some data that would work using a dot plot map (cruise ships) but doesn't work well with a filled map. The key to getting Power BI to recognize all of the county names correctly when you place them into the location field is to combine info into a format like \"Anoka County, Minnesota\" and to realize that the state of Louisiana has parishes and not counties. If you skip that piece of formating with the standard map you'll run into issues with common county names like Cook County, Georgia and Cook County, Minnesota. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"time_series_covid19_confirmed_US.csv\")\n",
    "df = df[(df.FIPS > 1000) &(df.FIPS < 57000)]\n",
    "df.loc[df.Province_State == 'Louisiana', 'County'] = df.loc[df.Province_State == 'Louisiana', 'Admin2'] + ' Parish, ' +  df.loc[df.Province_State == 'Louisiana', 'Province_State']\n",
    "df.loc[df.Province_State != 'Louisiana', 'County'] = df.loc[df.Province_State != 'Louisiana', 'Admin2'] + ' County, ' +  df.loc[df.Province_State != 'Louisiana', 'Province_State']\n",
    "df = df.iloc[:,-2:]\n",
    "df.rename(columns = {df.columns[0]: 'Confirmed Cases'}, inplace = True)\n",
    "df_cc = df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"time_series_covid19_deaths_US.csv\")\n",
    "df = df[(df.FIPS > 1000) &(df.FIPS < 57000)]\n",
    "df.loc[df.Province_State == 'Louisiana', 'County'] = df.loc[df.Province_State == 'Louisiana', 'Admin2'] + ' Parish, ' +  df.loc[df.Province_State == 'Louisiana', 'Province_State']\n",
    "df.loc[df.Province_State != 'Louisiana', 'County'] = df.loc[df.Province_State != 'Louisiana', 'Admin2'] + ' County, ' +  df.loc[df.Province_State != 'Louisiana', 'Province_State']\n",
    "df = df.iloc[:,-2:]\n",
    "df.rename(columns = {df.columns[0]: 'Deaths'}, inplace = True)\n",
    "\n",
    "df = pd.merge(df, df_cc)\n",
    "\n",
    "df['Natural Log of Current Cases'] = np.log(df.loc[:,'Confirmed Cases'] + 1)\n",
    "\n",
    "df.to_csv('county_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to pull some tweets to create a replica twitter timeline using the \"Timeline by Cloudscope\" visual. In order to pull from Twitter you must first set up a developer account. I've chosen to use the Twython package but a number of alternatives exist. Tweepy is quite popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_tweets = Twython(\"Your Twitter API key\", \"Your secret key\")\n",
    "\n",
    "query = {'q': 'covid',\n",
    "        'result_type': 'mixed',\n",
    "        'count': 100,\n",
    "        'lang': 'en',\n",
    "        }\n",
    "\n",
    "results = python_tweets.search(**query)\n",
    "\n",
    "dict_ = {'user': [],'name': [], 'date': [], 'text': [], 'profile_picture': [],\n",
    "         'favorite_count': [], 'retweet_count': [],'tweetJSON': [], 'truncated': []}\n",
    "\n",
    "for status in results['statuses']:\n",
    "    dict_['user'].append(status['user']['screen_name'])\n",
    "    dict_['name'].append(status['user']['name'])\n",
    "    dict_['date'].append(status['created_at'])\n",
    "    dict_['text'].append(status['text'])\n",
    "    dict_['profile_picture'].append(status['user']['profile_image_url'])\n",
    "    dict_['favorite_count'].append(status['favorite_count'])\n",
    "    dict_['retweet_count'].append(status['retweet_count'])\n",
    "    dict_['tweetJSON'].append(json.dumps(status))\n",
    "    dict_['truncated'].append(status['truncated'])\n",
    "    \n",
    "df_twitter = pd.DataFrame(dict_)\n",
    "\n",
    "df_twitter['formated_date'] = pd.to_datetime(df_twitter.date).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "df_twitter.to_csv('covid_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
